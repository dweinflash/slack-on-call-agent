# How to Resolve Kafka Outbound Message Backlog Issues

## What This Alert Indicates

This alert triggers when the 209731-VDF-LVDS-INT-KAFKA subscription in the LVDS (Low Velocity Data Streaming) application has a message backlog greater than 5,000 messages. This suggests the Vehicle Kafka Persistence module in LVDS is having trouble consuming and processing messages from the LVDSINTERNAL topic.

## When This Alert Triggers

**System**: Vehicle Kafka Persistence module  
**Severity**: Minor (P3)  
**Alert Duration**: 1,200 seconds (20 minutes)  
**Scope**: 209731-VDF-LVDS-INT-KAFKA subscription  
**Normal Range**: 500 to 2,500 messages  
**Alert Threshold**: Greater than 5,000 messages  
**Topic Location**: `persistent://209731-VDF/CC-NA-LVDS/LVDSINTERNAL`

## Common Causes

- **Patching Issue**: Data center or platform offline for maintenance (Kafka maintenance affects persistence module)
- **Deployment Issue**: Kubernetes pods with Vehicle Kafka Persistence module failing to start
- **Authentication Issue**: Module unable to connect to Kafka due to credential problems
- **Resource Constraints**: Insufficient processing capacity for message volume
- **Kafka Platform Issues**: Downstream Kafka system problems affecting message delivery

## How to Resolve This Issue

### Initial Assessment

1. **Check current message backlog**
   - Access Internal Consumption Backlog graph
   - Look for `persistent://209731-VDF/CC-NA-LVDS/LVDSINTERNAL --> 209731-VDF-LVDS-INT-KAFKA`
   - Verify current backlog size compared to normal range (500-2,500 messages)

### Primary Resolution - Scaling Up Services

2. **Scale up Springfield data center first**
   - Navigate to K8s Commands Pipeline
   - Click **Run Pipeline** in top right
   - Configure pipeline settings:
     - **Branch**: `main`
     - **Region**: `na`
     - **Team**: `delivery`
     - **Data Center**: `springfield`
     - **Environment**: `vdf-prodk2`
     - **Service**: `vehicle-profile-lb-service`
     - **Command**: `scale`
     - **Replicas**: `20`
     - **Pod**: `20`

3. **Execute scaling operation**
   - Click **Run**
   - Click **Review and Approve** when Review button appears
   - Wait for green check mark next to "Scale vehicle-kafka-persistence"

4. **Monitor backlog improvement**
   - Return to Internal Consumption Backlog graph
   - Click subscription in graph legend to highlight backlog
   - Check if backlog is decreasing after scaling

### Secondary Resolution - Riverside Data Center

5. **If Springfield scaling doesn't work**
   - Revert Springfield scaling: repeat steps 2-3 with **Replicas**: `15` and **Pod**: `15`
   - Scale Riverside data center: repeat steps 2-3 with **Data Center**: `riverside`
   - Monitor backlog improvement again

### Escalation Path

6. **If scaling doesn't resolve the issue**
   - Revert all scaling changes back to normal levels
   - Issue is likely not patching-related
   - Reassign ticket to assignment group `LVDS.Support`
   - Include scaling attempts and results in ticket notes

## If These Steps Don't Work

If the above steps do not resolve the issue:

1. Escalate the ticket to the **LVDS.Support** assignment group
2. Include all scaling operations attempted and their results
3. Provide current backlog metrics and trends
4. Include authentication and deployment status checks
5. Consider investigating Vehicle Kafka Persistence module logs

For immediate assistance, contact the LVDS.Support team with scaling results and current system status.